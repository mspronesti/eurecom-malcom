{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# MALCOM Lab Session 1\n",
    "\n",
    "# Neural Network Implementation and Training\n",
    "In this lab session, we will develop a simple neural network with fully-connected layers to perform classification, and test it out on the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from malcom.neural_net import TwoLayerNet\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "We will use the class `TwoLayerNet` in the file `malcom/neural_net.py` to represent instances of our network. The network parameters are stored in the instance variable `self.params` where keys are string parameter names and values are numpy arrays. We will start with initializing toy data and a toy model, which we will use to further develop the neural network implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 2 1]\n"
     ]
    }
   ],
   "source": [
    "# We create a small network and some toy data to check our implementations.\n",
    "# Random seed is set for repeatable experiments.\n",
    "\n",
    "input_size = 4\n",
    "hidden_size = 10\n",
    "num_classes = 3\n",
    "num_inputs = 5\n",
    "\n",
    "def init_toy_model():\n",
    "    np.random.seed(0)\n",
    "    return TwoLayerNet(input_size, hidden_size, num_classes, std=1e-1)\n",
    "\n",
    "def init_toy_data():\n",
    "    np.random.seed(1)\n",
    "    X = 10 * np.random.randn(num_inputs, input_size)\n",
    "    y = np.array([0, 1, 2, 2, 1])\n",
    "    return X, y\n",
    "\n",
    "net = init_toy_model()\n",
    "X, y = init_toy_data()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward pass: compute scores\n",
    "Open the file `malcom/neural_net.py` and look at the method `TwoLayerNet.loss`. This function takes the data and weights and computes the class scores, the loss, and the gradients on the parameters. \n",
    "\n",
    "Implement the first part of the forward pass which uses the weights and biases to compute the scores for all inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your scores:\n",
      "[[-0.81233741 -1.27654624 -0.70335995]\n",
      " [-0.17129677 -1.18803311 -0.47310444]\n",
      " [-0.51590475 -1.01354314 -0.8504215 ]\n",
      " [-0.15419291 -0.48629638 -0.52901952]\n",
      " [-0.00618733 -0.12435261 -0.15226949]]\n",
      "\n",
      "Correct scores:\n",
      "[[-0.81233741 -1.27654624 -0.70335995]\n",
      " [-0.17129677 -1.18803311 -0.47310444]\n",
      " [-0.51590475 -1.01354314 -0.8504215 ]\n",
      " [-0.15419291 -0.48629638 -0.52901952]\n",
      " [-0.00618733 -0.12435261 -0.15226949]]\n",
      "\n",
      "Difference between your scores and correct scores:\n",
      "3.6802720745909845e-08\n"
     ]
    }
   ],
   "source": [
    "scores = net.loss(X)\n",
    "print('Your scores:')\n",
    "print(scores)\n",
    "print()\n",
    "print('Correct scores:')\n",
    "correct_scores = np.asarray([\n",
    "  [-0.81233741, -1.27654624, -0.70335995],\n",
    "  [-0.17129677, -1.18803311, -0.47310444],\n",
    "  [-0.51590475, -1.01354314, -0.8504215 ],\n",
    "  [-0.15419291, -0.48629638, -0.52901952],\n",
    "  [-0.00618733, -0.12435261, -0.15226949]])\n",
    "print(correct_scores)\n",
    "print()\n",
    "\n",
    "# The difference should be very small, less than 1e-7\n",
    "print('Difference between your scores and correct scores:')\n",
    "print(np.sum(np.abs(scores - correct_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward pass: compute loss\n",
    "In the same function, implement the second part that computes the data and the regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between your loss and correct loss:\n",
      "1.7985612998927536e-13\n"
     ]
    }
   ],
   "source": [
    "loss, _ = net.loss(X, y, reg=0.05)\n",
    "correct_loss = 1.30378789133\n",
    "\n",
    "# This should be very small, less than 1e-12\n",
    "print('Difference between your loss and correct loss:')\n",
    "print(np.sum(np.abs(loss - correct_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward pass\n",
    "Implement the rest of the function. This will compute the gradient of the loss with respect to the variables `W1`, `b1`, `W2`, and `b2`. To check that the forward pass has been implemented correctly, you can debug your backward pass using a numeric gradient check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 max relative error: 3.561318e-09\n",
      "b1 max relative error: 2.738421e-09\n",
      "W2 max relative error: 3.440708e-09\n",
      "b2 max relative error: 4.447646e-11\n"
     ]
    }
   ],
   "source": [
    "from malcom.gradient_check import eval_numerical_gradient\n",
    "\n",
    "# Use numeric gradient checking to check your implementation of the backward pass.\n",
    "# If your implementation is correct, the difference between the numeric and\n",
    "# the analytic gradients should be less than 1e-8 for each of W1, W2, b1, and b2.\n",
    "\n",
    "loss, grads = net.loss(X, y, reg=0.05)\n",
    "\n",
    "# these should all be less than 1e-8 or so\n",
    "for param_name in grads:\n",
    "    f = lambda W: net.loss(X, y, reg=0.05)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, net.params[param_name], verbose=False)\n",
    "    print('%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Training\n",
    "To train the neural network, we will use stochastic gradient descent (SGD). Look at the function `TwoLayerNet.train` and fill in the missing sections to implement the training procedure. \n",
    "\n",
    "You will also have to implement `TwoLayerNet.predict`, as the training process periodically performs prediction to keep track of accuracy over time while the network trains.\n",
    "\n",
    "Once you have implemented the method, run the code below to train a two-layer network on some toy data. The expected training loss should be less than 0.02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training loss:  0.0007311789655197494\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "net = init_toy_model()\n",
    "stats = net.train(X, y, X, y,\n",
    "            learning_rate=1e-1, reg=5e-6, batch_size=3,\n",
    "            num_iters=100, verbose=False)\n",
    "\n",
    "print('Final training loss: ', stats['loss_history'][-1])\n",
    "\n",
    "# plot the loss history\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('training loss')\n",
    "plt.title('Training Loss history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 Data Loading\n",
    "We will now train the implemented neural network in a real dataset, the widely used CIFAR-10.\n",
    "You can download this dataset at: https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "Then, you can put it in the folder malcom/datasets (or anywhere else you want, but then you will need to change the path in the below code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "from malcom.data_utils import load_CIFAR10\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n",
    "    \"\"\"\n",
    "    This will allows us to load the CIFAR-10 dataset and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'malcom/datasets/cifar-10-batches-py'\n",
    "    \n",
    "    # Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "    try:\n",
    "       del X_train, y_train\n",
    "       del X_test, y_test\n",
    "       print('Clear previously loaded data.')\n",
    "    except:\n",
    "       pass\n",
    "\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "        \n",
    "    # Subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    # Reshape data to rows\n",
    "    X_train = X_train.reshape(num_training, -1)\n",
    "    X_val = X_val.reshape(num_validation, -1)\n",
    "    X_test = X_test.reshape(num_test, -1)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Training\n",
    "To train our network we will use (as before) SGD. In addition, we will adjust the learning rate with an exponential learning rate schedule as optimization proceeds; after each epoch, we will reduce the learning rate by multiplying it by a decay rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "input_size = 32 * 32 * 3\n",
    "hidden_size = 50\n",
    "num_classes = 10\n",
    "net = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Train the network\n",
    "stats = net.train(X_train, y_train, X_val, y_val,\n",
    "            num_iters=1000, batch_size=200,\n",
    "            learning_rate=1e-4, learning_rate_decay=0.95,\n",
    "            reg=0.25, verbose=True)\n",
    "\n",
    "# Predict on the validation set\n",
    "val_acc = (net.predict(X_val) == y_val).mean()\n",
    "print('Validation accuracy: ', val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Debugging\n",
    "With the default parameters set above, the validation accuracy should be around 0.29 on the validation set. This is a relatively poor performance.\n",
    "\n",
    "One strategy to understand where is the problem is to plot the loss function and the accuracies on the training and validation sets during optimization.\n",
    "\n",
    "Another strategy is to visualize the weights that were learned in the first layer of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss function and train / validation accuracies\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(stats['train_acc_history'], label='train')\n",
    "plt.plot(stats['val_acc_history'], label='val')\n",
    "plt.title('Classification accuracy history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Classification accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from malcom.vis_utils import visualize_grid\n",
    "\n",
    "# Visualize the weights of the network\n",
    "\n",
    "def show_net_weights(net):\n",
    "    W1 = net.params['W1']\n",
    "    W1 = W1.reshape(32, 32, 3, -1).transpose(3, 0, 1, 2)\n",
    "    plt.imshow(visualize_grid(W1, padding=3).astype('uint8'))\n",
    "    plt.gca().axis('off')\n",
    "    plt.show()\n",
    "\n",
    "show_net_weights(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters tuning\n",
    "\n",
    "**What's wrong?**. Looking at the visualizations above, we observe that the loss is decreasing more or less linearly, which seems to suggest that the learning rate may be too low. Moreover, there is no gap between the training and validation accuracy, suggesting that the model we used has low capacity, and that we should increase its size. On the other hand, with a very large model we would expect to see more overfitting, which would manifest itself as a very large gap between the training and validation accuracy.\n",
    "\n",
    "**Tuning**. Tuning the hyperparameters and developing intuition for how these hyperparameters affect the final performance is an important task in Neural Networks - one would need a lot of practice and experience to get this skill. \n",
    "Here, we will experiment with different values of the various hyperparameters, including hidden layer size, learning rate, number of training epochs, and regularization strength. \n",
    "You might also consider tuning the learning rate decay (but even without this, you should be able to get good performance).\n",
    "\n",
    "**Target accuracy**. The target classification accuracy to be achieved should be greater than 48% on the validation set. \n",
    "\n",
    "**Experiment**: The goal here is to get the best result possible on CIFAR-10. The best performance, with a more elaborated network, should be around 52%. \n",
    "Students with prior ML background could also implement various techniques to improve performance (e.g. PCA to reduce dimensionality, dropout, adding features to the solver, etc.). This is not needed though for getting good performance, unless we want to really approach the reference accuracy value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "**Explain your hyperparameter tuning process below.**\n",
    "\n",
    "$\\color{blue}{\\textit {Your Answer}}$ TO BE COMPLETED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# TODO: Tune hyperparameters using the validation set. Store your best trained  #\n",
    "# model in best_net.                                                            #\n",
    "#                                                                               #\n",
    "# To help debug your network, it may help to use visualizations similar to the  #\n",
    "# ones we used above; these visualizations will have significant qualitative    #\n",
    "# differences from the ones we saw above for the poorly tuned network.          #\n",
    "#                                                                               #\n",
    "#################################################################################\n",
    "# *****START OF YOUR CODE*****\n",
    "\n",
    "input_size = 32 * 32 * 3\n",
    "hidden_size = 200 # increased number of neurons to increase the capacity of the network\n",
    "num_classes = 10\n",
    "best_net = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Train the network\n",
    "stats = best_net.train(X_train, y_train, X_val, y_val,\n",
    "            num_iters=3000, batch_size=200,\n",
    "            learning_rate=1e-3, learning_rate_decay=0.95,\n",
    "            reg=0.25, verbose=True)\n",
    "\n",
    "# increased number of iterations and increased the learning rate\n",
    "\n",
    "# Predict on the validation set\n",
    "val_acc = (best_net.predict(X_val) == y_val).mean()\n",
    "print('Validation accuracy: ', val_acc)\n",
    "\n",
    "# Plot the loss function and train / validation accuracies\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(stats['train_acc_history'], label='train')\n",
    "plt.plot(stats['val_acc_history'], label='val')\n",
    "plt.title('Classification accuracy history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Classification accuracy')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# *****END OF YOUR CODE*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualize the weights of the best network\n",
    "show_net_weights(best_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run on the test set\n",
    "When you are done experimenting, you should evaluate your final trained neural network on the test set. We expect to get accuracy above 48%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = (best_net.predict(X_test) == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = (best_net.predict(X_train) == y_train).mean()\n",
    "print('Training accuracy: ', train_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "**Question**\n",
    "\n",
    "Now that you have trained a Neural Network classifier, you may find that your testing accuracy is much lower than the training accuracy. Which of the below (possibly multiple) ways can decrease this gap?\n",
    "\n",
    "1. Increase the regularization strength.\n",
    "2. Train on a larger dataset.\n",
    "3. Add more hidden units.\n",
    "4. None of the above.\n",
    "\n",
    "$\\color{blue}{\\textit{Your Answer - Please Explain}}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
